{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11088f21",
   "metadata": {},
   "source": [
    "# hospital data curation project\n",
    "## phase 6: data validation and quality assurance\n",
    "\n",
    "comprehensive validation using:\n",
    "- business rule validation\n",
    "- data quality checks\n",
    "- referential integrity verification\n",
    "- automated validation reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c751ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# add src directory to python path\n",
    "notebook_dir = Path(os.getcwd())\n",
    "src_dir = notebook_dir / 'src'\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import project modules\n",
    "import config\n",
    "import validators\n",
    "import utils\n",
    "\n",
    "# use imported modules\n",
    "CLEANED_DATA_DIR = config.CLEANED_DATA_DIR\n",
    "PREPROCESSED_DATA_DIR = config.PREPROCESSED_DATA_DIR\n",
    "LOGS_DIR = config.LOGS_DIR\n",
    "DataValidator = validators.DataValidator\n",
    "setup_logging = utils.setup_logging\n",
    "print_section_header = utils.print_section_header\n",
    "load_dataframe = utils.load_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3014126",
   "metadata": {},
   "source": [
    "## 1. load datasets for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e47638c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:51:44,488 - root - INFO - loaded dataframe from d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_patients.csv: 3000 rows, 7 columns\n",
      "2025-11-10 17:51:44,499 - root - INFO - loaded dataframe from d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_visits.csv: 2481 rows, 8 columns\n",
      "2025-11-10 17:51:44,499 - root - INFO - loaded dataframe from d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_visits.csv: 2481 rows, 8 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:51:44,524 - root - INFO - loaded dataframe from d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_diagnoses.csv: 7998 rows, 5 columns\n",
      "2025-11-10 17:51:44,564 - root - INFO - loaded dataframe from d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\preprocessed\\transformed_master_dataset.csv: 2481 rows, 36 columns\n",
      "2025-11-10 17:51:44,564 - root - INFO - loaded dataframe from d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\preprocessed\\transformed_master_dataset.csv: 2481 rows, 36 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                        loading datasets for validation                         \n",
      "================================================================================\n",
      "\n",
      "datasets loaded for validation\n"
     ]
    }
   ],
   "source": [
    "# setup logging\n",
    "logger = setup_logging()\n",
    "\n",
    "# load datasets\n",
    "print_section_header(\"loading datasets for validation\")\n",
    "\n",
    "patients_df = load_dataframe(CLEANED_DATA_DIR / 'clean_patients.csv')\n",
    "visits_df = load_dataframe(CLEANED_DATA_DIR / 'clean_visits.csv')\n",
    "diagnoses_df = load_dataframe(CLEANED_DATA_DIR / 'clean_diagnoses.csv')\n",
    "master_df = load_dataframe(PREPROCESSED_DATA_DIR / 'transformed_master_dataset.csv')\n",
    "\n",
    "print(\"datasets loaded for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7cacf9",
   "metadata": {},
   "source": [
    "## 2. validate patients dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "328d61e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:51:44,588 - utils - INFO - validating patients data...\n",
      "2025-11-10 17:51:44,591 - utils - INFO - patients validation completed: 2/2 checks passed\n",
      "2025-11-10 17:51:44,591 - utils - INFO - patients validation completed: 2/2 checks passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                          validating patients dataset                           \n",
      "================================================================================\n",
      "\n",
      "\n",
      "validation results:\n",
      "✓ PASS | unique_patient_id\n",
      "       all patient ids are unique\n",
      "\n",
      "✓ PASS | no_missing_patient_id\n",
      "       no missing values in patient_id\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize validator\n",
    "validator = DataValidator()\n",
    "\n",
    "# validate patients data\n",
    "print_section_header(\"validating patients dataset\")\n",
    "patients_results = validator.validate_patients(patients_df)\n",
    "\n",
    "# display results\n",
    "print(\"\\nvalidation results:\")\n",
    "for check in patients_results['checks']:\n",
    "    status = \"✓ PASS\" if check['passed'] else \"✗ FAIL\"\n",
    "    print(f\"{status} | {check['test']}\")\n",
    "    print(f\"       {check['message']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c12902",
   "metadata": {},
   "source": [
    "## 3. validate visits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f06ee977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:51:44,602 - utils - INFO - validating visits data...\n",
      "2025-11-10 17:51:44,602 - utils - INFO - visits validation completed: 3/3 checks passed\n",
      "2025-11-10 17:51:44,602 - utils - INFO - visits validation completed: 3/3 checks passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                           validating visits dataset                            \n",
      "================================================================================\n",
      "\n",
      "\n",
      "validation results:\n",
      "✓ PASS | unique_visit_id\n",
      "       all visit ids are unique\n",
      "\n",
      "✓ PASS | discharge_after_admission\n",
      "       all discharge dates are after admission dates\n",
      "\n",
      "✓ PASS | valid_length_of_stay\n",
      "       all length of stay values are between 0 and 365 days\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate visits data\n",
    "print_section_header(\"validating visits dataset\")\n",
    "visits_results = validator.validate_visits(visits_df)\n",
    "\n",
    "# display results\n",
    "print(\"\\nvalidation results:\")\n",
    "for check in visits_results['checks']:\n",
    "    status = \"✓ PASS\" if check['passed'] else \"✗ FAIL\"\n",
    "    print(f\"{status} | {check['test']}\")\n",
    "    print(f\"       {check['message']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d62c4",
   "metadata": {},
   "source": [
    "## 4. validate diagnoses dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07cda708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:51:44,648 - utils - INFO - validating diagnoses data...\n",
      "2025-11-10 17:51:44,664 - utils - INFO - diagnoses validation completed: 2/2 checks passed\n",
      "2025-11-10 17:51:44,664 - utils - INFO - diagnoses validation completed: 2/2 checks passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                          validating diagnoses dataset                          \n",
      "================================================================================\n",
      "\n",
      "\n",
      "validation results:\n",
      "✓ PASS | valid_icd10_format\n",
      "       all icd-10 codes are valid\n",
      "\n",
      "✓ PASS | no_missing_visit_id\n",
      "       no missing visit ids\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate diagnoses data\n",
    "print_section_header(\"validating diagnoses dataset\")\n",
    "diagnoses_results = validator.validate_diagnoses(diagnoses_df)\n",
    "\n",
    "# display results\n",
    "print(\"\\nvalidation results:\")\n",
    "for check in diagnoses_results['checks']:\n",
    "    status = \"✓ PASS\" if check['passed'] else \"✗ FAIL\"\n",
    "    print(f\"{status} | {check['test']}\")\n",
    "    print(f\"       {check['message']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f454df9",
   "metadata": {},
   "source": [
    "## 5. validate referential integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e1b91be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:51:44,677 - utils - INFO - validating referential integrity...\n",
      "2025-11-10 17:51:44,686 - utils - INFO - referential integrity validation completed: 1/1 checks passed\n",
      "2025-11-10 17:51:44,686 - utils - INFO - referential integrity validation completed: 1/1 checks passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                        validating referential integrity                        \n",
      "================================================================================\n",
      "\n",
      "\n",
      "validation results:\n",
      "✓ PASS | visits_patient_id_exists\n",
      "       all patient ids in visits exist in patients table\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate referential integrity between datasets\n",
    "print_section_header(\"validating referential integrity\")\n",
    "integrity_results = validator.validate_referential_integrity(patients_df, visits_df)\n",
    "\n",
    "# display results\n",
    "print(\"\\nvalidation results:\")\n",
    "for check in integrity_results['checks']:\n",
    "    status = \"✓ PASS\" if check['passed'] else \"✗ FAIL\"\n",
    "    print(f\"{status} | {check['test']}\")\n",
    "    print(f\"       {check['message']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9d2c3",
   "metadata": {},
   "source": [
    "## 6. validate transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a25444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                        validating transformed features                         \n",
      "================================================================================\n",
      "\n",
      "\n",
      "validation results:\n",
      "✓ PASS | readmission_flag_valid\n",
      "       found 117 readmissions\n",
      "\n",
      "✓ PASS | high_risk_flag_binary\n",
      "       high risk flag is binary\n",
      "\n",
      "✓ PASS | los_category_consistency\n",
      "       los categories match length of stay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate transformed dataset features\n",
    "print_section_header(\"validating transformed features\")\n",
    "\n",
    "validation_checks = []\n",
    "\n",
    "# check readmission logic\n",
    "if 'is_readmitted' in master_df.columns:\n",
    "    total_readmissions = master_df['is_readmitted'].sum()\n",
    "    check_passed = total_readmissions >= 0\n",
    "    validation_checks.append({\n",
    "        'test': 'readmission_flag_valid',\n",
    "        'passed': check_passed,\n",
    "        'message': f'found {total_readmissions} readmissions'\n",
    "    })\n",
    "\n",
    "# check age groups\n",
    "if 'age_group' in master_df.columns:\n",
    "    valid_groups = ['0-18', '19-35', '36-60', '60+', 'unknown']\n",
    "    invalid_groups = ~master_df['age_group'].isin(valid_groups)\n",
    "    check_passed = invalid_groups.sum() == 0\n",
    "    validation_checks.append({\n",
    "        'test': 'age_group_categories_valid',\n",
    "        'passed': check_passed,\n",
    "        'message': 'all age groups are valid' if check_passed else f'found {invalid_groups.sum()} invalid age groups'\n",
    "    })\n",
    "\n",
    "# check high risk flag\n",
    "if 'is_high_risk' in master_df.columns:\n",
    "    valid_values = master_df['is_high_risk'].isin([0, 1]).all()\n",
    "    validation_checks.append({\n",
    "        'test': 'high_risk_flag_binary',\n",
    "        'passed': valid_values,\n",
    "        'message': 'high risk flag is binary' if valid_values else 'high risk flag contains invalid values'\n",
    "    })\n",
    "\n",
    "# check length of stay consistency\n",
    "if 'length_of_stay' in master_df.columns and 'los_category' in master_df.columns:\n",
    "    # verify los_category matches length_of_stay\n",
    "    inconsistent = 0\n",
    "    for idx, row in master_df.iterrows():\n",
    "        los = row['length_of_stay']\n",
    "        category = row['los_category']\n",
    "        if pd.notna(los) and pd.notna(category):\n",
    "            if los <= 1 and category != 'short_stay':\n",
    "                inconsistent += 1\n",
    "            elif 1 < los <= 7 and category != 'medium_stay':\n",
    "                inconsistent += 1\n",
    "            elif 7 < los <= 14 and category != 'long_stay':\n",
    "                inconsistent += 1\n",
    "            elif los > 14 and category != 'extended_stay':\n",
    "                inconsistent += 1\n",
    "    \n",
    "    check_passed = inconsistent == 0\n",
    "    validation_checks.append({\n",
    "        'test': 'los_category_consistency',\n",
    "        'passed': check_passed,\n",
    "        'message': 'los categories match length of stay' if check_passed else f'found {inconsistent} inconsistent categorizations'\n",
    "    })\n",
    "\n",
    "# display results\n",
    "print(\"\\nvalidation results:\")\n",
    "for check in validation_checks:\n",
    "    status = \"✓ PASS\" if check['passed'] else \"✗ FAIL\"\n",
    "    print(f\"{status} | {check['test']}\")\n",
    "    print(f\"       {check['message']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92229bc5",
   "metadata": {},
   "source": [
    "## 7. data completeness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35b86cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                           data completeness analysis                           \n",
      "================================================================================\n",
      "\n",
      "columns with less than 100% completeness:\n",
      "                   column  non_null_count  completeness_pct\n",
      "days_since_last_admission             789             31.80\n",
      "          all_medications            1731             69.77\n",
      "            all_diagnoses            2001             80.65\n",
      "        primary_diagnosis            2001             80.65\n",
      "\n",
      "completeness report saved to: d:\\Github Desktop\\Python\\Hospital Data Curation\\logs\\completeness_report.csv\n"
     ]
    }
   ],
   "source": [
    "# analyze data completeness\n",
    "print_section_header(\"data completeness analysis\")\n",
    "\n",
    "completeness_metrics = []\n",
    "\n",
    "for col in master_df.columns:\n",
    "    non_null = master_df[col].notna().sum()\n",
    "    completeness = (non_null / len(master_df)) * 100\n",
    "    \n",
    "    completeness_metrics.append({\n",
    "        'column': col,\n",
    "        'non_null_count': non_null,\n",
    "        'completeness_pct': round(completeness, 2)\n",
    "    })\n",
    "\n",
    "completeness_df = pd.DataFrame(completeness_metrics)\n",
    "completeness_df = completeness_df.sort_values('completeness_pct')\n",
    "\n",
    "print(\"columns with less than 100% completeness:\")\n",
    "incomplete = completeness_df[completeness_df['completeness_pct'] < 100]\n",
    "if len(incomplete) > 0:\n",
    "    print(incomplete.to_string(index=False))\n",
    "else:\n",
    "    print(\"all columns are 100% complete\")\n",
    "\n",
    "# save completeness report\n",
    "completeness_file = LOGS_DIR / 'completeness_report.csv'\n",
    "completeness_df.to_csv(completeness_file, index=False)\n",
    "print(f\"\\ncompleteness report saved to: {completeness_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eea6cb",
   "metadata": {},
   "source": [
    "## 8. statistical validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab47d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                             statistical validation                             \n",
      "================================================================================\n",
      "\n",
      "key statistical validations:\n",
      "\n",
      "✓ length of stay distribution valid: mean=182.27, median=180.00\n",
      "✓ average diagnoses per visit: 1.63\n",
      "✓ average medications per visit: 1.06\n",
      "✓ readmission rate: 4.72%\n",
      "✓ high-risk patient rate: 21.93%\n"
     ]
    }
   ],
   "source": [
    "# perform statistical validation\n",
    "print_section_header(\"statistical validation\")\n",
    "\n",
    "print(\"key statistical validations:\\n\")\n",
    "\n",
    "# validate age distribution\n",
    "if 'age' in master_df.columns:\n",
    "    age_mean = master_df['age'].mean()\n",
    "    age_std = master_df['age'].std()\n",
    "    age_valid = (age_mean > 0) and (age_std > 0)\n",
    "    print(f\"✓ age distribution valid: mean={age_mean:.1f}, std={age_std:.1f}\")\n",
    "\n",
    "# validate length of stay distribution\n",
    "if 'length_of_stay' in master_df.columns:\n",
    "    los_mean = master_df['length_of_stay'].mean()\n",
    "    los_median = master_df['length_of_stay'].median()\n",
    "    los_valid = (los_mean > 0) and (los_median > 0)\n",
    "    print(f\"✓ length of stay distribution valid: mean={los_mean:.2f}, median={los_median:.2f}\")\n",
    "\n",
    "# validate diagnosis and medication counts\n",
    "if 'diagnosis_count' in master_df.columns:\n",
    "    diag_mean = master_df['diagnosis_count'].mean()\n",
    "    print(f\"✓ average diagnoses per visit: {diag_mean:.2f}\")\n",
    "\n",
    "if 'medication_count' in master_df.columns:\n",
    "    med_mean = master_df['medication_count'].mean()\n",
    "    print(f\"✓ average medications per visit: {med_mean:.2f}\")\n",
    "\n",
    "# validate readmission rate\n",
    "if 'is_readmitted' in master_df.columns:\n",
    "    readmission_rate = (master_df['is_readmitted'].sum() / len(master_df)) * 100\n",
    "    rate_valid = 0 <= readmission_rate <= 50  # reasonable range\n",
    "    status = \"✓\" if rate_valid else \"⚠\"\n",
    "    print(f\"{status} readmission rate: {readmission_rate:.2f}%\")\n",
    "\n",
    "# validate high-risk patient proportion\n",
    "if 'is_high_risk' in master_df.columns:\n",
    "    high_risk_rate = (master_df['is_high_risk'].sum() / len(master_df)) * 100\n",
    "    print(f\"✓ high-risk patient rate: {high_risk_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ae810",
   "metadata": {},
   "source": [
    "## 9. generate comprehensive validation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c4d2fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:51:44,847 - utils - INFO - validation report saved to d:\\Github Desktop\\Python\\Hospital Data Curation\\logs\\validation_report.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                          generating validation report                          \n",
      "================================================================================\n",
      "\n",
      "\n",
      "validation report generated:\n",
      "================================================================================\n",
      "DATA VALIDATION REPORT\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Dataset: patients\n",
      "--------------------------------------------------------------------------------\n",
      "✓ PASS | unique_patient_id\n",
      "       all patient ids are unique\n",
      "\n",
      "✓ PASS | no_missing_patient_id\n",
      "       no missing values in patient_id\n",
      "\n",
      "\n",
      "Dataset: visits\n",
      "--------------------------------------------------------------------------------\n",
      "✓ PASS | unique_visit_id\n",
      "       all visit ids are unique\n",
      "\n",
      "✓ PASS | discharge_after_admission\n",
      "       all discharge dates are after admission dates\n",
      "\n",
      "✓ PASS | valid_length_of_stay\n",
      "       all length of stay values are between 0 and 365 days\n",
      "\n",
      "\n",
      "Dataset: diagnoses\n",
      "--------------------------------------------------------------------------------\n",
      "✓ PASS | valid_icd10_format\n",
      "       all icd-10 codes are valid\n",
      "\n",
      "✓ PASS | no_missing_visit_id\n",
      "       no missing visit ids\n",
      "\n",
      "\n",
      "Dataset: referential_integrity\n",
      "--------------------------------------------------------------------------------\n",
      "✓ PASS | visits_patient_id_exists\n",
      "       all patient ids in visits exist in patients table\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: 8/8 checks passed (100.0%)\n",
      "================================================================================\n",
      "\n",
      "validation report saved to: d:\\Github Desktop\\Python\\Hospital Data Curation\\logs\\validation_report.txt\n"
     ]
    }
   ],
   "source": [
    "# generate and save validation report\n",
    "print_section_header(\"generating validation report\")\n",
    "\n",
    "report_text = validator.generate_validation_report()\n",
    "print(\"\\nvalidation report generated:\")\n",
    "print(report_text)\n",
    "\n",
    "print(f\"\\nvalidation report saved to: {LOGS_DIR / 'validation_report.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba097e8",
   "metadata": {},
   "source": [
    "## 10. final data quality scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12bd66d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                             data quality scorecard                             \n",
      "================================================================================\n",
      "\n",
      "                   metric status details\n",
      "    patient id uniqueness ✓ pass    True\n",
      "      visit id uniqueness ✓ pass    True\n",
      "overall data completeness ✓ pass  96.19%\n",
      "         date consistency ✓ pass    True\n",
      "\n",
      "quality scorecard saved to: d:\\Github Desktop\\Python\\Hospital Data Curation\\logs\\quality_scorecard.csv\n"
     ]
    }
   ],
   "source": [
    "# create data quality scorecard\n",
    "print_section_header(\"data quality scorecard\")\n",
    "\n",
    "scorecard = {\n",
    "    'metric': [],\n",
    "    'status': [],\n",
    "    'details': []\n",
    "}\n",
    "\n",
    "# uniqueness\n",
    "if 'patient_id' in patients_df.columns:\n",
    "    is_unique = patients_df['patient_id'].is_unique\n",
    "    scorecard['metric'].append('patient id uniqueness')\n",
    "    scorecard['status'].append('✓ pass' if is_unique else '✗ fail')\n",
    "    scorecard['details'].append(f\"{is_unique}\")\n",
    "\n",
    "if 'visit_id' in visits_df.columns:\n",
    "    is_unique = visits_df['visit_id'].is_unique\n",
    "    scorecard['metric'].append('visit id uniqueness')\n",
    "    scorecard['status'].append('✓ pass' if is_unique else '✗ fail')\n",
    "    scorecard['details'].append(f\"{is_unique}\")\n",
    "\n",
    "# completeness\n",
    "overall_completeness = (master_df.notna().sum().sum() / (len(master_df) * len(master_df.columns))) * 100\n",
    "scorecard['metric'].append('overall data completeness')\n",
    "scorecard['status'].append('✓ pass' if overall_completeness > 90 else '⚠ warning')\n",
    "scorecard['details'].append(f\"{overall_completeness:.2f}%\")\n",
    "\n",
    "# consistency - check date columns\n",
    "if 'discharge_date' in visits_df.columns and 'admission_date' in visits_df.columns:\n",
    "    # convert to datetime if not already\n",
    "    visits_df['admission_date'] = pd.to_datetime(visits_df['admission_date'], errors='coerce')\n",
    "    visits_df['discharge_date'] = pd.to_datetime(visits_df['discharge_date'], errors='coerce')\n",
    "    date_consistency = (visits_df['discharge_date'] >= visits_df['admission_date']).all()\n",
    "    scorecard['metric'].append('date consistency')\n",
    "    scorecard['status'].append('✓ pass' if date_consistency else '✗ fail')\n",
    "    scorecard['details'].append(str(date_consistency))\n",
    "\n",
    "# validity - check age if exists in master dataset\n",
    "if 'age' in master_df.columns:\n",
    "    age_validity = master_df['age'].between(0, 120).all()\n",
    "    scorecard['metric'].append('age validity')\n",
    "    scorecard['status'].append('✓ pass' if age_validity else '✗ fail')\n",
    "    scorecard['details'].append(str(age_validity))\n",
    "elif 'age' in patients_df.columns:\n",
    "    age_validity = patients_df['age'].between(0, 120).all()\n",
    "    scorecard['metric'].append('age validity')\n",
    "    scorecard['status'].append('✓ pass' if age_validity else '✗ fail')\n",
    "    scorecard['details'].append(str(age_validity))\n",
    "\n",
    "# create scorecard dataframe\n",
    "scorecard_df = pd.DataFrame(scorecard)\n",
    "print(scorecard_df.to_string(index=False))\n",
    "\n",
    "# save scorecard\n",
    "scorecard_file = LOGS_DIR / 'quality_scorecard.csv'\n",
    "scorecard_df.to_csv(scorecard_file, index=False)\n",
    "print(f\"\\nquality scorecard saved to: {scorecard_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bc7bc",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "data validation completed:\n",
    "- ✓ patients dataset validated\n",
    "- ✓ visits dataset validated\n",
    "- ✓ diagnoses dataset validated\n",
    "- ✓ referential integrity confirmed\n",
    "- ✓ transformed features validated\n",
    "- ✓ completeness analysis performed\n",
    "- ✓ statistical validation completed\n",
    "- ✓ quality scorecard generated\n",
    "\n",
    "all validation reports saved to `logs/`\n",
    "\n",
    "next phase: predictive analytics (regression, association, classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
