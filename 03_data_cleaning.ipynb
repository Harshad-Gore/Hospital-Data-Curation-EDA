{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375bdadf",
   "metadata": {},
   "source": [
    "# hospital data curation project\n",
    "## phase 3: data cleaning and standardization\n",
    "\n",
    "comprehensive data cleaning including:\n",
    "- schema alignment and standardization\n",
    "- missing value imputation\n",
    "- duplicate removal\n",
    "- outlier detection and handling\n",
    "- data type corrections\n",
    "- business rule validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28954b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# add src directory to python path\n",
    "notebook_dir = Path(os.getcwd())\n",
    "src_dir = notebook_dir / 'src'\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import project modules\n",
    "import config\n",
    "import data_loader\n",
    "import data_cleaner\n",
    "import utils\n",
    "\n",
    "# use imported modules\n",
    "RAW_DATA_DIR = config.RAW_DATA_DIR\n",
    "CLEANED_DATA_DIR = config.CLEANED_DATA_DIR\n",
    "LOGS_DIR = config.LOGS_DIR\n",
    "DataLoader = data_loader.DataLoader\n",
    "DataCleaner = data_cleaner.DataCleaner\n",
    "setup_logging = utils.setup_logging\n",
    "print_section_header = utils.print_section_header\n",
    "save_dataframe = utils.save_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666b4a3",
   "metadata": {},
   "source": [
    "## 1. load raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e34537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:47:19,322 - utils - INFO - successfully loaded patients.csv: 3000 rows, 7 columns\n",
      "2025-11-10 17:47:19,324 - utils - INFO - loaded patients: 3000 rows\n",
      "2025-11-10 17:47:19,324 - utils - INFO - loaded patients: 3000 rows\n",
      "2025-11-10 17:47:19,342 - utils - INFO - successfully loaded visits.csv: 5000 rows, 7 columns\n",
      "2025-11-10 17:47:19,346 - utils - INFO - loaded visits: 5000 rows\n",
      "2025-11-10 17:47:19,342 - utils - INFO - successfully loaded visits.csv: 5000 rows, 7 columns\n",
      "2025-11-10 17:47:19,346 - utils - INFO - loaded visits: 5000 rows\n",
      "2025-11-10 17:47:19,373 - utils - INFO - successfully loaded diagnoses.csv: 8000 rows, 4 columns\n",
      "2025-11-10 17:47:19,374 - utils - INFO - loaded diagnoses: 8000 rows\n",
      "2025-11-10 17:47:19,373 - utils - INFO - successfully loaded diagnoses.csv: 8000 rows, 4 columns\n",
      "2025-11-10 17:47:19,374 - utils - INFO - loaded diagnoses: 8000 rows\n",
      "2025-11-10 17:47:19,395 - utils - INFO - successfully loaded medications.csv: 6000 rows, 6 columns\n",
      "2025-11-10 17:47:19,397 - utils - INFO - loaded medications: 6000 rows\n",
      "2025-11-10 17:47:19,400 - utils - INFO - successfully loaded staff.csv: 500 rows, 5 columns\n",
      "2025-11-10 17:47:19,400 - utils - INFO - loaded staff: 500 rows\n",
      "2025-11-10 17:47:19,395 - utils - INFO - successfully loaded medications.csv: 6000 rows, 6 columns\n",
      "2025-11-10 17:47:19,397 - utils - INFO - loaded medications: 6000 rows\n",
      "2025-11-10 17:47:19,400 - utils - INFO - successfully loaded staff.csv: 500 rows, 5 columns\n",
      "2025-11-10 17:47:19,400 - utils - INFO - loaded staff: 500 rows\n",
      "2025-11-10 17:47:19,412 - utils - INFO - successfully loaded hospital_info.csv: 20 rows, 5 columns\n",
      "2025-11-10 17:47:19,412 - utils - INFO - loaded hospital_info: 20 rows\n",
      "2025-11-10 17:47:19,412 - utils - INFO - successfully loaded hospital_info.csv: 20 rows, 5 columns\n",
      "2025-11-10 17:47:19,412 - utils - INFO - loaded hospital_info: 20 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                          data cleaning initialization                          \n",
      "================================================================================\n",
      "\n",
      "datasets loaded: ['patients', 'visits', 'diagnoses', 'medications', 'staff', 'hospital_info']\n",
      "output directory: d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\n"
     ]
    }
   ],
   "source": [
    "# setup logging\n",
    "logger = setup_logging()\n",
    "\n",
    "# load datasets\n",
    "loader = DataLoader(data_dir=RAW_DATA_DIR)\n",
    "datasets = loader.load_all_datasets()\n",
    "\n",
    "print_section_header(\"data cleaning initialization\")\n",
    "print(f\"datasets loaded: {list(datasets.keys())}\")\n",
    "print(f\"output directory: {CLEANED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0733d1",
   "metadata": {},
   "source": [
    "## 2. clean patients dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7b03b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:47:19,442 - utils - INFO - cleaning patients data...\n",
      "2025-11-10 17:47:19,449 - utils - INFO - standardized gender values\n",
      "2025-11-10 17:47:19,450 - utils - INFO - removed 0 duplicate patient records\n",
      "2025-11-10 17:47:19,450 - utils - INFO - patients data cleaned: 3000 -> 3000 rows\n",
      "2025-11-10 17:47:19,449 - utils - INFO - standardized gender values\n",
      "2025-11-10 17:47:19,450 - utils - INFO - removed 0 duplicate patient records\n",
      "2025-11-10 17:47:19,450 - utils - INFO - patients data cleaned: 3000 -> 3000 rows\n",
      "2025-11-10 17:47:19,483 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_patients.csv\n",
      "2025-11-10 17:47:19,483 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_patients.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                           cleaning patients dataset                            \n",
      "================================================================================\n",
      "\n",
      "initial shape: (3000, 7)\n",
      "\n",
      "before cleaning:\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "\n",
      "after cleaning:\n",
      "  final shape: (3000, 7)\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# initialize cleaner\n",
    "cleaner = DataCleaner()\n",
    "\n",
    "# clean patients data\n",
    "if 'patients' in datasets:\n",
    "    print_section_header(\"cleaning patients dataset\")\n",
    "    \n",
    "    patients_df = datasets['patients'].copy()\n",
    "    print(f\"initial shape: {patients_df.shape}\")\n",
    "    \n",
    "    # display before cleaning\n",
    "    print(\"\\nbefore cleaning:\")\n",
    "    print(f\"  missing values: {patients_df.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {patients_df.duplicated().sum()}\")\n",
    "    \n",
    "    # clean patients data\n",
    "    patients_clean = cleaner.clean_patients_data(patients_df)\n",
    "    \n",
    "    # display after cleaning\n",
    "    print(\"\\nafter cleaning:\")\n",
    "    print(f\"  final shape: {patients_clean.shape}\")\n",
    "    print(f\"  missing values: {patients_clean.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {patients_clean.duplicated().sum()}\")\n",
    "    \n",
    "    # save cleaned data\n",
    "    save_dataframe(patients_clean, CLEANED_DATA_DIR / 'clean_patients.csv')\n",
    "    \n",
    "    # store for later use\n",
    "    datasets['patients'] = patients_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b5642",
   "metadata": {},
   "source": [
    "## 3. clean visits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c789ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:47:19,504 - utils - INFO - cleaning visits data...\n",
      "2025-11-10 17:47:19,525 - utils - INFO - removed 1254 records with discharge before admission\n",
      "2025-11-10 17:47:19,533 - utils - INFO - removed 1265 records with invalid length of stay\n",
      "2025-11-10 17:47:19,538 - utils - INFO - removed 0 duplicate visit records\n",
      "2025-11-10 17:47:19,525 - utils - INFO - removed 1254 records with discharge before admission\n",
      "2025-11-10 17:47:19,533 - utils - INFO - removed 1265 records with invalid length of stay\n",
      "2025-11-10 17:47:19,538 - utils - INFO - removed 0 duplicate visit records\n",
      "2025-11-10 17:47:19,540 - utils - INFO - visits data cleaned: 5000 -> 2481 rows\n",
      "2025-11-10 17:47:19,540 - utils - INFO - visits data cleaned: 5000 -> 2481 rows\n",
      "2025-11-10 17:47:19,564 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_visits.csv\n",
      "2025-11-10 17:47:19,564 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_visits.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                            cleaning visits dataset                             \n",
      "================================================================================\n",
      "\n",
      "initial shape: (5000, 7)\n",
      "\n",
      "before cleaning:\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "\n",
      "after cleaning:\n",
      "  final shape: (2481, 8)\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "\n",
      "length of stay statistics:\n",
      "  mean: 182.27 days\n",
      "  median: 180.00 days\n",
      "  range: 0 - 365 days\n"
     ]
    }
   ],
   "source": [
    "# clean visits data\n",
    "if 'visits' in datasets:\n",
    "    print_section_header(\"cleaning visits dataset\")\n",
    "    \n",
    "    visits_df = datasets['visits'].copy()\n",
    "    print(f\"initial shape: {visits_df.shape}\")\n",
    "    \n",
    "    # display before cleaning\n",
    "    print(\"\\nbefore cleaning:\")\n",
    "    print(f\"  missing values: {visits_df.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {visits_df.duplicated().sum()}\")\n",
    "    \n",
    "    # clean visits data\n",
    "    visits_clean = cleaner.clean_visits_data(visits_df)\n",
    "    \n",
    "    # display after cleaning\n",
    "    print(\"\\nafter cleaning:\")\n",
    "    print(f\"  final shape: {visits_clean.shape}\")\n",
    "    print(f\"  missing values: {visits_clean.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {visits_clean.duplicated().sum()}\")\n",
    "    \n",
    "    if 'length_of_stay' in visits_clean.columns:\n",
    "        print(f\"\\nlength of stay statistics:\")\n",
    "        print(f\"  mean: {visits_clean['length_of_stay'].mean():.2f} days\")\n",
    "        print(f\"  median: {visits_clean['length_of_stay'].median():.2f} days\")\n",
    "        print(f\"  range: {visits_clean['length_of_stay'].min()} - {visits_clean['length_of_stay'].max()} days\")\n",
    "    \n",
    "    # save cleaned data\n",
    "    save_dataframe(visits_clean, CLEANED_DATA_DIR / 'clean_visits.csv')\n",
    "    \n",
    "    # store for later use\n",
    "    datasets['visits'] = visits_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68f4a7",
   "metadata": {},
   "source": [
    "## 4. clean diagnoses dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3449bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:47:19,584 - utils - INFO - cleaning diagnoses data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                           cleaning diagnoses dataset                           \n",
      "================================================================================\n",
      "\n",
      "initial shape: (8000, 4)\n",
      "\n",
      "before cleaning:\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:47:19,601 - utils - INFO - found 0 invalid icd-10 codes\n",
      "2025-11-10 17:47:19,620 - utils - INFO - removed 2 duplicate diagnosis records\n",
      "2025-11-10 17:47:19,620 - utils - INFO - diagnoses data cleaned: 8000 -> 7998 rows\n",
      "2025-11-10 17:47:19,620 - utils - INFO - removed 2 duplicate diagnosis records\n",
      "2025-11-10 17:47:19,620 - utils - INFO - diagnoses data cleaned: 8000 -> 7998 rows\n",
      "2025-11-10 17:47:19,675 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_diagnoses.csv\n",
      "2025-11-10 17:47:19,675 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_diagnoses.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after cleaning:\n",
      "  final shape: (7998, 5)\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "\n",
      "icd-10 code validation:\n",
      "  valid codes: 7998/7998 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# clean diagnoses data\n",
    "if 'diagnoses' in datasets:\n",
    "    print_section_header(\"cleaning diagnoses dataset\")\n",
    "    \n",
    "    diagnoses_df = datasets['diagnoses'].copy()\n",
    "    print(f\"initial shape: {diagnoses_df.shape}\")\n",
    "    \n",
    "    # display before cleaning\n",
    "    print(\"\\nbefore cleaning:\")\n",
    "    print(f\"  missing values: {diagnoses_df.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {diagnoses_df.duplicated().sum()}\")\n",
    "    \n",
    "    # clean diagnoses data\n",
    "    diagnoses_clean = cleaner.clean_diagnoses_data(diagnoses_df)\n",
    "    \n",
    "    # display after cleaning\n",
    "    print(\"\\nafter cleaning:\")\n",
    "    print(f\"  final shape: {diagnoses_clean.shape}\")\n",
    "    print(f\"  missing values: {diagnoses_clean.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {diagnoses_clean.duplicated().sum()}\")\n",
    "    \n",
    "    if 'icd_code_valid' in diagnoses_clean.columns:\n",
    "        valid_count = diagnoses_clean['icd_code_valid'].sum()\n",
    "        total_count = len(diagnoses_clean)\n",
    "        print(f\"\\nicd-10 code validation:\")\n",
    "        print(f\"  valid codes: {valid_count}/{total_count} ({valid_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    # save cleaned data\n",
    "    save_dataframe(diagnoses_clean, CLEANED_DATA_DIR / 'clean_diagnoses.csv')\n",
    "    \n",
    "    # store for later use\n",
    "    datasets['diagnoses'] = diagnoses_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea471d3",
   "metadata": {},
   "source": [
    "## 5. clean medications dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831df154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:47:19,706 - utils - INFO - cleaning medications data...\n",
      "2025-11-10 17:47:19,721 - utils - INFO - removed 676 duplicate medication records\n",
      "2025-11-10 17:47:19,722 - utils - INFO - medications data cleaned: 6000 -> 5324 rows\n",
      "2025-11-10 17:47:19,721 - utils - INFO - removed 676 duplicate medication records\n",
      "2025-11-10 17:47:19,722 - utils - INFO - medications data cleaned: 6000 -> 5324 rows\n",
      "2025-11-10 17:47:19,749 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_medications.csv\n",
      "2025-11-10 17:47:19,749 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_medications.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                          cleaning medications dataset                          \n",
      "================================================================================\n",
      "\n",
      "initial shape: (6000, 6)\n",
      "\n",
      "before cleaning:\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "\n",
      "after cleaning:\n",
      "  final shape: (5324, 6)\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  unique medications: 5\n"
     ]
    }
   ],
   "source": [
    "# clean medications data\n",
    "if 'medications' in datasets:\n",
    "    print_section_header(\"cleaning medications dataset\")\n",
    "    \n",
    "    medications_df = datasets['medications'].copy()\n",
    "    print(f\"initial shape: {medications_df.shape}\")\n",
    "    \n",
    "    # display before cleaning\n",
    "    print(\"\\nbefore cleaning:\")\n",
    "    print(f\"  missing values: {medications_df.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {medications_df.duplicated().sum()}\")\n",
    "    \n",
    "    # clean medications data\n",
    "    medications_clean = cleaner.clean_medications_data(medications_df)\n",
    "    \n",
    "    # display after cleaning\n",
    "    print(\"\\nafter cleaning:\")\n",
    "    print(f\"  final shape: {medications_clean.shape}\")\n",
    "    print(f\"  missing values: {medications_clean.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {medications_clean.duplicated().sum()}\")\n",
    "    print(f\"  unique medications: {medications_clean['medication_name'].nunique()}\")\n",
    "    \n",
    "    # save cleaned data\n",
    "    save_dataframe(medications_clean, CLEANED_DATA_DIR / 'clean_medications.csv')\n",
    "    \n",
    "    # store for later use\n",
    "    datasets['medications'] = medications_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed37b87",
   "metadata": {},
   "source": [
    "## 6. clean staff dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bbfd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:47:19,770 - utils - INFO - cleaning staff data...\n",
      "2025-11-10 17:47:19,775 - utils - INFO - removed 0 duplicate staff records\n",
      "2025-11-10 17:47:19,777 - utils - INFO - staff data cleaned: 500 -> 500 rows\n",
      "2025-11-10 17:47:19,775 - utils - INFO - removed 0 duplicate staff records\n",
      "2025-11-10 17:47:19,777 - utils - INFO - staff data cleaned: 500 -> 500 rows\n",
      "2025-11-10 17:47:19,786 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_staff.csv\n",
      "2025-11-10 17:47:19,786 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_staff.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                             cleaning staff dataset                             \n",
      "================================================================================\n",
      "\n",
      "initial shape: (500, 5)\n",
      "\n",
      "before cleaning:\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "\n",
      "after cleaning:\n",
      "  final shape: (500, 5)\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# clean staff data\n",
    "if 'staff' in datasets:\n",
    "    print_section_header(\"cleaning staff dataset\")\n",
    "    \n",
    "    staff_df = datasets['staff'].copy()\n",
    "    print(f\"initial shape: {staff_df.shape}\")\n",
    "    \n",
    "    # display before cleaning\n",
    "    print(\"\\nbefore cleaning:\")\n",
    "    print(f\"  missing values: {staff_df.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {staff_df.duplicated().sum()}\")\n",
    "    \n",
    "    # clean staff data\n",
    "    staff_clean = cleaner.clean_staff_data(staff_df)\n",
    "    \n",
    "    # display after cleaning\n",
    "    print(\"\\nafter cleaning:\")\n",
    "    print(f\"  final shape: {staff_clean.shape}\")\n",
    "    print(f\"  missing values: {staff_clean.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {staff_clean.duplicated().sum()}\")\n",
    "    \n",
    "    # save cleaned data\n",
    "    save_dataframe(staff_clean, CLEANED_DATA_DIR / 'clean_staff.csv')\n",
    "    \n",
    "    # store for later use\n",
    "    datasets['staff'] = staff_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d313f",
   "metadata": {},
   "source": [
    "## 7. clean hospital_info dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f8a8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         cleaning hospital_info dataset                         \n",
      "================================================================================\n",
      "\n",
      "initial shape: (20, 5)\n",
      "final shape: (20, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:47:19,810 - root - INFO - saved dataframe to d:\\Github Desktop\\Python\\Hospital Data Curation\\data\\cleaned\\clean_hospital_info.csv\n"
     ]
    }
   ],
   "source": [
    "# clean hospital_info data (if exists)\n",
    "if 'hospital_info' in datasets:\n",
    "    print_section_header(\"cleaning hospital_info dataset\")\n",
    "    \n",
    "    hospital_df = datasets['hospital_info'].copy()\n",
    "    print(f\"initial shape: {hospital_df.shape}\")\n",
    "    \n",
    "    # basic cleaning (remove duplicates, handle missing)\n",
    "    hospital_clean = hospital_df.drop_duplicates()\n",
    "    \n",
    "    print(f\"final shape: {hospital_clean.shape}\")\n",
    "    \n",
    "    # save cleaned data\n",
    "    save_dataframe(hospital_clean, CLEANED_DATA_DIR / 'clean_hospital_info.csv')\n",
    "    \n",
    "    datasets['hospital_info'] = hospital_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e7814",
   "metadata": {},
   "source": [
    "## 8. generate cleaning summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c21a9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                            cleaning summary report                             \n",
      "================================================================================\n",
      "\n",
      "    dataset  initial_rows  final_rows  removed_rows\n",
      "   patients          3000        3000             0\n",
      "     visits          5000        2481          2519\n",
      "  diagnoses          8000        7998             2\n",
      "medications          6000        5324           676\n",
      "      staff           500         500             0\n",
      "\n",
      "cleaning report saved to: d:\\Github Desktop\\Python\\Hospital Data Curation\\logs\\cleaning_report.csv\n",
      "\n",
      "overall statistics:\n",
      "  total initial rows: 22500\n",
      "  total final rows: 19303\n",
      "  total removed rows: 3197 (14.21%)\n"
     ]
    }
   ],
   "source": [
    "# generate comprehensive cleaning report\n",
    "print_section_header(\"cleaning summary report\")\n",
    "\n",
    "cleaning_report = cleaner.get_cleaning_report()\n",
    "print(cleaning_report.to_string(index=False))\n",
    "\n",
    "# save cleaning report\n",
    "report_file = LOGS_DIR / 'cleaning_report.csv'\n",
    "cleaning_report.to_csv(report_file, index=False)\n",
    "print(f\"\\ncleaning report saved to: {report_file}\")\n",
    "\n",
    "# calculate overall statistics\n",
    "total_initial = cleaning_report['initial_rows'].sum()\n",
    "total_final = cleaning_report['final_rows'].sum()\n",
    "total_removed = cleaning_report['removed_rows'].sum()\n",
    "\n",
    "print(f\"\\noverall statistics:\")\n",
    "print(f\"  total initial rows: {total_initial}\")\n",
    "print(f\"  total final rows: {total_final}\")\n",
    "print(f\"  total removed rows: {total_removed} ({total_removed/total_initial*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c566b",
   "metadata": {},
   "source": [
    "## 9. final data quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eb9f73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                            final data quality check                            \n",
      "================================================================================\n",
      "\n",
      "\n",
      "PATIENTS:\n",
      "  rows: 3000\n",
      "  columns: 7\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  memory usage: 1.48 mb\n",
      "\n",
      "VISITS:\n",
      "  rows: 2481\n",
      "  columns: 8\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  memory usage: 0.81 mb\n",
      "\n",
      "DIAGNOSES:\n",
      "  rows: 7998\n",
      "  columns: 5\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  memory usage: 2.11 mb\n",
      "\n",
      "MEDICATIONS:\n",
      "  rows: 5324\n",
      "  columns: 6\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  memory usage: 2.02 mb\n",
      "\n",
      "STAFF:\n",
      "  rows: 500\n",
      "  columns: 5\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  memory usage: 0.15 mb\n",
      "\n",
      "HOSPITAL_INFO:\n",
      "  rows: 20\n",
      "  columns: 5\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  memory usage: 0.00 mb\n",
      "  duplicate rows: 0\n",
      "  memory usage: 2.02 mb\n",
      "\n",
      "STAFF:\n",
      "  rows: 500\n",
      "  columns: 5\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  memory usage: 0.15 mb\n",
      "\n",
      "HOSPITAL_INFO:\n",
      "  rows: 20\n",
      "  columns: 5\n",
      "  missing values: 0\n",
      "  duplicate rows: 0\n",
      "  memory usage: 0.00 mb\n"
     ]
    }
   ],
   "source": [
    "# perform final quality checks on cleaned data\n",
    "print_section_header(\"final data quality check\")\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    print(f\"\\n{dataset_name.upper()}:\")\n",
    "    print(f\"  rows: {len(df)}\")\n",
    "    print(f\"  columns: {len(df.columns)}\")\n",
    "    print(f\"  missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  duplicate rows: {df.duplicated().sum()}\")\n",
    "    print(f\"  memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438f6e1",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "data cleaning completed:\n",
    "- ✓ patients dataset cleaned and standardized\n",
    "- ✓ visits dataset cleaned with valid date ranges\n",
    "- ✓ diagnoses dataset cleaned with icd-10 validation\n",
    "- ✓ medications dataset cleaned and normalized\n",
    "- ✓ staff dataset cleaned\n",
    "- ✓ all cleaned datasets saved to `data/cleaned/`\n",
    "\n",
    "next phase: data integration and merging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
